\contentsline {section}{\numberline {1}线性回归与梯度下降法}{5}{}%
\contentsline {subsection}{\numberline {1.1}线性回归模型定义}{5}{}%
\contentsline {subsection}{\numberline {1.2}误差函数}{5}{}%
\contentsline {subsection}{\numberline {1.3}穷举法}{5}{}%
\contentsline {subsection}{\numberline {1.4}最小二乘法}{5}{}%
\contentsline {subsubsection}{\numberline {1.4.1}简介}{5}{}%
\contentsline {subsubsection}{\numberline {1.4.2}向量形式求解}{6}{}%
\contentsline {subsection}{\numberline {1.5}梯度下降法}{8}{}%
\contentsline {section}{\numberline {2}逻辑回归算法模型}{9}{}%
\contentsline {subsection}{\numberline {2.1}逻辑回归算法原理}{9}{}%
\contentsline {subsubsection}{\numberline {2.1.1}逻辑回归算法公式}{9}{}%
\contentsline {subsubsection}{\numberline {2.1.2}交叉熵损失函数}{10}{}%
\contentsline {subsubsection}{\numberline {2.1.3}参数w的更新}{11}{}%
\contentsline {subsection}{\numberline {2.2}参数b的更新}{11}{}%
\contentsline {subsection}{\numberline {2.3}回归和分类的区别}{12}{}%
\contentsline {subsection}{\numberline {2.4}分类模型评价指标}{12}{}%
\contentsline {subsubsection}{\numberline {2.4.1}准确率（Accuracy）}{12}{}%
\contentsline {subsubsection}{\numberline {2.4.2}精确率（Precision）}{12}{}%
\contentsline {subsubsection}{\numberline {2.4.3}召回率（Recall）}{13}{}%
\contentsline {subsubsection}{\numberline {2.4.4}F1 值}{13}{}%
\contentsline {subsection}{\numberline {2.5}回归模型评价指标}{13}{}%
\contentsline {subsubsection}{\numberline {2.5.1}平均绝对误差(MAE)}{13}{}%
\contentsline {subsubsection}{\numberline {2.5.2}均方误差(MSE)}{13}{}%
\contentsline {subsubsection}{\numberline {2.5.3}均方根误差(RMSE)}{13}{}%
\contentsline {subsubsection}{\numberline {2.5.4}评价绝对百分比误差（MAPE）}{14}{}%
\contentsline {subsection}{\numberline {2.6}训练集、验证集与测试集}{14}{}%
\contentsline {section}{\numberline {3}全连接神经网络}{15}{}%
\contentsline {subsection}{\numberline {3.1}全连接神经网络架构}{15}{}%
\contentsline {subsection}{\numberline {3.2}激活函数}{16}{}%
\contentsline {subsubsection}{\numberline {3.2.1}为什么要导入激活函数}{16}{}%
\contentsline {subsection}{\numberline {3.3}常见激活函数}{16}{}%
\contentsline {subsubsection}{\numberline {3.3.1}Sigmoid函数}{16}{}%
\contentsline {subsubsection}{\numberline {3.3.2}Tanh函数}{17}{}%
\contentsline {subsubsection}{\numberline {3.3.3}ReLU函数}{18}{}%
\contentsline {subsubsection}{\numberline {3.3.4}Leaky ReLU函数}{19}{}%
\contentsline {subsubsection}{\numberline {3.3.5}SoftMax激活函数}{20}{}%
\contentsline {subsection}{\numberline {3.4}前向传播}{20}{}%
\contentsline {subsection}{\numberline {3.5}反向传播}{20}{}%
\contentsline {section}{\numberline {4}卷积神经网络}{22}{}%
\contentsline {subsection}{\numberline {4.1}简介}{22}{}%
\contentsline {subsubsection}{\numberline {4.1.1}基本结构组成}{22}{}%
\contentsline {subsubsection}{\numberline {4.1.2}应用场景}{23}{}%
\contentsline {subsubsection}{\numberline {4.1.3}经典模型}{23}{}%
\contentsline {subsubsection}{\numberline {4.1.4}核心优势}{23}{}%
\contentsline {subsection}{\numberline {4.2}卷积层}{24}{}%
\contentsline {subsubsection}{\numberline {4.2.1}卷积核}{24}{}%
\contentsline {subsubsection}{\numberline {4.2.2}卷积运算}{24}{}%
\contentsline {subsubsection}{\numberline {4.2.3}步长(Stride)}{25}{}%
\contentsline {subsubsection}{\numberline {4.2.4}填充(Padding)}{25}{}%
\contentsline {subsubsection}{\numberline {4.2.5}参数共享}{25}{}%
\contentsline {subsubsection}{\numberline {4.2.6}多通道卷积运算}{26}{}%
\contentsline {subsection}{\numberline {4.3}池化层}{26}{}%
\contentsline {subsubsection}{\numberline {4.3.1}核心作用}{27}{}%
\contentsline {subsubsection}{\numberline {4.3.2}常见池化类型}{27}{}%
\contentsline {subsubsection}{\numberline {4.3.3}输出尺寸计算}{27}{}%
\contentsline {subsubsection}{\numberline {4.3.4}特点总结}{27}{}%
\contentsline {section}{\numberline {5}循环神经网络}{28}{}%
\contentsline {subsection}{\numberline {5.1}RNN简介}{28}{}%
\contentsline {subsection}{\numberline {5.2}RNN结构}{28}{}%
\contentsline {subsubsection}{\numberline {5.2.1}折叠起来的循环视角}{28}{}%
\contentsline {subsubsection}{\numberline {5.2.2}按照时间展开的序列视角}{29}{}%
\contentsline {subsubsection}{\numberline {5.2.3}RNN整体结构}{29}{}%
\contentsline {subsection}{\numberline {5.3}RNN数学模型}{29}{}%
\contentsline {subsubsection}{\numberline {5.3.1}前向传播}{29}{}%
\contentsline {subsubsection}{\numberline {5.3.2}随时间反向传播（BPTT）}{30}{}%
\contentsline {subsection}{\numberline {5.4}RNN梯度爆炸和梯度消失}{31}{}%
\contentsline {subsubsection}{\numberline {5.4.1}梯度爆炸 (Exploding Gradients)}{31}{}%
\contentsline {subsubsection}{\numberline {5.4.2}梯度消失 (Vanishing Gradients)}{31}{}%
\contentsline {subsubsection}{\numberline {5.4.3}解决方案}{32}{}%
\contentsline {subsection}{\numberline {5.5}LSTM基本架构与原理}{32}{}%
\contentsline {subsubsection}{\numberline {5.5.1}记忆细胞}{32}{}%
\contentsline {subsubsection}{\numberline {5.5.2}遗忘门}{34}{}%
\contentsline {subsubsection}{\numberline {5.5.3}输入门}{34}{}%
\contentsline {subsubsection}{\numberline {5.5.4}更新细胞状态}{35}{}%
\contentsline {subsubsection}{\numberline {5.5.5}输出门}{35}{}%
\contentsline {subsection}{\numberline {5.6}LSTM反向传播以及缓解梯度消失和梯度爆炸}{36}{}%
